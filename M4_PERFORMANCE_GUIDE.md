# M4 Mac Performance Guide for LLM Integration

This guide is specifically optimized for M4 Macs with different resource configurations.

## üñ•Ô∏è **M4 Mac Specifications**

| Model | RAM | Storage | Performance Rating | Recommended Mode |
|-------|-----|---------|-------------------|------------------|
| **M4 MacBook Air/Pro** | 16GB+ | 512GB+ | ‚ö°‚ö°‚ö°‚ö°‚ö° | `llm` (multimodal) |

## üöÄ **Performance Mode**

### **Multimodal Mode** (`llm`)
**Best for: 16GB+ RAM, maximum performance**

```bash
# Download multimodal model
ollama pull llava

# Run full multimodal mode
python main.py -m llm
```

**Features:**
- ‚úÖ **Image + text processing**
- ‚úÖ **Best decision quality**
- ‚úÖ **Full visual context**
- ‚úÖ **~6GB RAM usage**
- ‚úÖ **Slower but smarter** (5-10 seconds)

**Performance:**
- LLM calls: 100% with image processing
- Response time: 5-10 seconds
- Memory usage: ~6GB

## üìä **Performance Comparison**

| Mode | RAM Usage | Speed | Quality | M4 Mac Suitability |
|------|-----------|-------|---------|-------------------|
| **llm** | ~6GB | ‚ö°‚ö°‚ö° | ‚ö°‚ö°‚ö°‚ö°‚ö° | Best for 16GB+ |

## üîß **M4-Specific Optimizations**

### **1. Use Apple Silicon Optimizations**
Ollama automatically uses M4's Neural Engine and GPU acceleration.

### **2. Memory Management**
```bash
# Monitor memory usage
top -pid $(pgrep ollama)

# Restart Ollama if needed
pkill ollama && ollama serve
```

### **3. Storage Optimization**
```bash
# Check available models
ollama list

# Remove unused models
ollama rm llama2:13b  # If you have larger models you don't use
```

## üéØ **Recommended Setup by M4 Model**

### **M4 MacBook Air/Pro (16GB+ RAM)**
```bash
# 1. Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Download multimodal model
ollama pull llava

# 3. Run full multimodal mode
python main.py -m llm
```

## ‚ö° **Performance Tips**

### **1. Monitor Performance**
```bash
# Check if Ollama is using GPU
ollama ps

# Monitor memory usage
htop  # or Activity Monitor on Mac
```

### **2. Optimize for Your Use Case**
- **Best Quality**: Use `llm` (multimodal)

## üö® **Troubleshooting M4 Issues**

### **"Out of Memory" Errors**
```bash
# Restart Ollama
pkill ollama
ollama serve
```

### **Slow Performance**
```bash
# Check if GPU is being used
ollama ps

# Restart with fresh memory
pkill ollama && ollama serve
```

### **Model Download Issues**
```bash
# Check internet connection
ping ollama.ai

# Try different model
ollama pull llava
```

## üìà **Expected Performance on M4**

| Task | llm |
|------|-----|
| **Game completion** | 5-8 minutes |
| **Memory usage** | 6GB |
| **CPU usage** | 60-70% |
| **Battery impact** | High |

## üéÆ **Gaming Experience**

### **Full Multimodal Mode**
- **Speed**: Slower but more thoughtful
- **Quality**: Best decision-making
- **Battery**: Higher impact

## üí° **Pro Tips for M4**

1. **Use Activity Monitor** to track memory usage
2. **Restart Ollama** if performance degrades
3. **Keep models updated** for best performance 